# -*- coding: utf-8 -*-
"""ML Algorithms.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12uH91_ckZ8xH4yea87q2wsuh1o6iOuRB
"""

pip install imblearn

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from imblearn.over_sampling import SMOTE
from collections import Counter

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import make_pipeline

from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier

from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, f1_score, recall_score, classification_report
from sklearn.metrics import plot_confusion_matrix

# %matplotlib inline

data = pd.read_csv('breast_cancer_DS.csv')
data.head()

data.shape

data.describe

data.info()

data.isnull().sum()

"""## There is not a single null value present in this dataset
### EDA
### Univariate Analysis
### Data Imbalance
"""

sns.countplot(x='feature_names', data=data)

data['feature_names'].value_counts()

"""There is not much data imbalance present
So, we can proceed further without much caring for data imbalance
Or We can handle it by many ways - upsampling/downsampling
## We will go one by one feature
As 'Id' is not carrying any important information for our problem so it's better to drop it
"""

data.drop(columns=['id'], inplace=True)

data.shape

diag_map = {
    "M":1,
    "B":0
}

data['feature_names'] = data['feature_names'].map(diag_map)

data[['feature_names']]

"""## Let's Plot the Histogram of remaining features to visualize their distribution"""

cols_name = list(data.columns)
cols_name

plt.figure(figsize = (25,55))
for i in range(1, len(cols_name)):
    plt.subplot(10, 3, i)
    sns.distplot(data[cols_name[i]])
plt.show()

"""Many attributes follow 'Normal / Gaussian Distribution'
Though many attributes have 'right skewness' present
We can use StandardScaler() to handle this data
## Let's plot Box plots for outliers visualization
"""

plt.figure(figsize = (25,55))
for i in range(1, len(cols_name)):
    plt.subplot(10, 3, i)
    sns.boxplot(data[cols_name[i]])
plt.show()

"""Yes, we thought right, There are outliers present
We will handle it with simple mathematics concept
## Bivariate Analysis
### Let's try to plot Corr
"""

import matplotlib.pyplot as plt
import seaborn as sns
plt.figure(figsize=(30, 20))
plt.title('Breast Cancer Feature Correlation', fontsize=50, ha='center')
sns.heatmap(data[cols_name[1:]].corr(), annot=True, square='square', fmt='.2g',linewidths=2)

"""There are some features which are highly corr with each others"""

corr_matrix = data[cols_name[1:]].corr()
mask = corr_matrix >= 0.9
mask

"""There are many 'False' values presents
Let's get all the columns pairs which are highly corr
"""

corr_pairs = []
for column1 in cols_name[1:]:
    for column2 in cols_name[1:]:
        if corr_matrix[column1][column2] >= 0 and column1!=column2:
            pair = (column1, column2)
            if pair not in corr_pairs and pair[::-1] not in corr_pairs:
                corr_pairs.append(pair)

corr_pairs

"""We can notice that the above mentioned pairs are highly correlated with pearson correlation value >= 0.
We can eliminate the columns that are highly correlated based on the correlation of the column with the target data.
"""

cols = list(set([col for i in corr_pairs for col in i]))

for col in cols:
    print('{} : {}'.format(col, data[col].corr(data['feature_names'])))

list_col = [col for col in cols if data[col].corr(data['feature_names'])>=0]

list_col

"""We will keep only columns that have corr>=0 with 'diagnosis' column"""

for col in cols:
    if col not in list_col:
        data.drop(columns=col, inplace = True)
data.shape

data.head()

X = data[data.columns[1:]]
y = data['feature_names']

"""## Without handle Outliers and Data Imbalanced
### Since we have algorithms available which are not sensitive to outliers and/or data imbalanced
"""

#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

X_train, X_test, y_train, y_test = train_test_split(X.values, y, test_size = 0.2, random_state = 42)

"""## Random - Forest Classifier"""

rf_pipeline = make_pipeline(StandardScaler(), RandomForestClassifier(random_state = 18))
rf_pipeline.fit(X_train, y_train)



# Accuracy On Whole Data
predictions = rf_pipeline.predict(X.values)
accuracy = accuracy_score(y, predictions)
print(f"Accuracy on Whole Data: {accuracy*100}%")
print(f"Precision Score: {precision_score(y, predictions)}")
print(f"Recall Score: {recall_score(y, predictions)}")
print(f"F1 Score: {f1_score(y, predictions)}")
plot_confusion_matrix(rf_pipeline, X.values, y)
plt.title("Confusion Matrix for Whole Data")
plt.show()
print(classification_report(y, predictions))

"""## XgBoost Classifier"""

xgb_pipeline = make_pipeline(StandardScaler(), XGBClassifier(random_state = 42))
xgb_pipeline.fit(X_train, y_train)



# Accuray On Whole Data
predictions = xgb_pipeline.predict(X.values)
accuracy = accuracy_score(y, predictions)
print(f"Accuracy on Whole Data: {accuracy*100}%")
print(f"Precision Score: {precision_score(y, predictions)}")
print(f"Recall Score: {recall_score(y, predictions)}")
print(f"F1 Score: {f1_score(y, predictions)}")
plot_confusion_matrix(xgb_pipeline, X.values, y)
plt.title("Confusion Matrix for Whole Data")
plt.show()
print(classification_report(y, predictions))

"""## Logistic Regression"""

from sklearn.linear_model import LogisticRegression

# Create a pipeline with StandardScaler and LogisticRegression
lr_pipeline = make_pipeline(StandardScaler(), LogisticRegression(random_state=18))

# Fit the pipeline on the training data
lr_pipeline.fit(X_train, y_train)



# Evaluate the pipeline on the whole data
predictions = lr_pipeline.predict(X.values)
accuracy = accuracy_score(y, predictions)
print(f"Accuracy on Whole Data: {accuracy*100}%")
print(f"Precision Score: {precision_score(y, predictions)}")
print(f"Recall Score: {recall_score(y,predictions)}")
print(f"F1 Score: {f1_score(y,predictions)}")
plot_confusion_matrix(lr_pipeline, X.values, y)
plt.title("Confusion Matrix for Whole Data")
plt.show()
print(classification_report(y,predictions))

"""## Decision Tree"""

#decision tree                                                                                                                                                                                                                                  from sklearn.tree import DecisionTreeClassifier
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, plot_confusion_matrix, classification_report
import matplotlib.pyplot as plt

# create pipeline with standard scaler and decision tree classifier
dt_pipeline = make_pipeline(StandardScaler(), DecisionTreeClassifier(random_state=18))

# fit the pipeline to the training data
dt_pipeline.fit(X_train, y_train)



# evaluate the model on the whole data
predictions = dt_pipeline.predict(X.values)
accuracy = accuracy_score(y,predictions)
precision = precision_score(y,predictions)
recall = recall_score(y,predictions)
f1 = f1_score(y, predictions)
print(f"Accuracy on Whole Data: {accuracy*100}%")
print(f"Precision Score: {precision}")
print(f"Recall Score: {recall}")
print(f"F1 Score: {f1}")
plot_confusion_matrix(dt_pipeline, X.values, y)
plt.title("Confusion Matrix for Whole Data")
plt.show()
print(classification_report(y,predictions))

"""## K- Nearest Neighbor"""

#KNN                                                                                                                                                                                                                                               from sklearn.neighbors import KNeighborsClassifier
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, plot_confusion_matrix, classification_report
import matplotlib.pyplot as plt

# Create a KNN pipeline with standardization
knn_pipeline = make_pipeline(StandardScaler(), KNeighborsClassifier(n_neighbors=5))

# Train the KNN model
knn_pipeline.fit(X_train, y_train)

# Evaluate the KNN model on whole data
predictions = knn_pipeline.predict(X.values)
accuracy = accuracy_score(y,predictions)
print(f"Accuracy on Whole Data: {accuracy*100}%")
print(f"Precision Score: {precision_score(y,predictions)}")
print(f"Recall Score: {recall_score(y,predictions)}")
print(f"F1 Score: {f1_score(y,predictions)}")
plot_confusion_matrix(knn_pipeline, X.values, y)
plt.title("Confusion Matrix for Whole Data")
plt.show()
print(classification_report(y,predictions))

import seaborn as sns
import matplotlib.pyplot as plt

# Define the models and corresponding accuracies
model = ['RandomForestClassifier', 'XGBClassifier', 'LogisticRegression','DecisionTreeClassifier','KNeighborsClassifier']

acc = [0.90001,0.70461,0.50574,0.89844,0.64939]

# Plot the barplot
plt.figure(figsize=[10, 5], dpi=100)
plt.title('Accuracy Comparison')
plt.xlabel('Algorithms')
plt.ylabel('Accuracy')
sns.barplot(x=model, y=acc)
plt.show()

accuracy_models = dict(zip(model, acc))
for k, v in accuracy_models.items():
    print (k, '-->', v*100)

import numpy as np
from sklearn.metrics import roc_curve, auc
import matplotlib.pyplot as plt

# Define the true labels
y_true = y  # Replace with your true labels

# Define the predicted probabilities for each model
proba_model1 = rf_pipeline.predict_proba(X.values)[:, 1]  # Replace with the appropriate model and input data
proba_model2 = xgb_pipeline.predict_proba(X.values)[:, 1]  # Replace with the appropriate model and input data
proba_model3 = lr_pipeline.predict_proba(X.values)[:, 1]  # Replace with the appropriate model and input data
proba_model4 = dt_pipeline.predict_proba(X.values)[:, 1]  # Replace with the appropriate model and input data
proba_model5 = knn_pipeline.predict_proba(X.values)[:, 1]  # Replace with the appropriate model and input data

# Compute the false positive rate (FPR), true positive rate (TPR), and area under the curve (AUC) for each model
fpr1, tpr1, _ = roc_curve(y_true, proba_model1)
roc_auc1 = auc(fpr1, tpr1)

fpr2, tpr2, _ = roc_curve(y_true, proba_model2)
roc_auc2 = auc(fpr2, tpr2)

fpr3, tpr3, _ = roc_curve(y_true, proba_model3)
roc_auc3 = auc(fpr3, tpr3)

fpr4, tpr4, _ = roc_curve(y_true, proba_model4)
roc_auc4 = auc(fpr4, tpr4)

fpr5, tpr5, _ = roc_curve(y_true, proba_model5)
roc_auc5 = auc(fpr5, tpr5)

# Plot the ROC curve for each model
plt.figure(figsize=[8, 6])
plt.plot(fpr1, tpr1, label=f'RF Model (AUC = {roc_auc1:.2f})')
plt.plot(fpr2, tpr2, label=f'XGB Model (AUC = {roc_auc2:.2f})')
plt.plot(fpr3, tpr3, label=f'LR Model (AUC = {roc_auc3:.2f})')
plt.plot(fpr4, tpr4, label=f'DT Model (AUC = {roc_auc4:.2f})')
plt.plot(fpr5, tpr5, label=f'KNN Model (AUC = {roc_auc5:.2f})')
plt.plot([0, 1], [0, 1], 'k--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic')
plt.legend(loc='lower right')
plt.show()

"""# ARTIFICIAL NEURAL NETWORKS

## Back Propagation Neural Network
"""

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

# Define the model architecture
model = Sequential()
model.add(Dense(32, input_dim=X_train.shape[1], activation='relu'))
model.add(Dense(1, activation='sigmoid'))

# Compile the model
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

# Train the model
history = model.fit(X_train, y_train, epochs=25, batch_size=32, validation_data=(X_test, y_test))

# Evaluate the model on the test data
loss, accuracy = model.evaluate(X_test, y_test)
print(f"Accuracy on Test Data: {accuracy*100}%")
# Plot the learning curve
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Model Accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper left')
plt.show()

"""## Multi Layer Perceptron"""

from sklearn.neural_network import MLPClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import make_pipeline
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, plot_confusion_matrix

# Create an MLP pipeline with standardization
mlp_pipeline = make_pipeline(StandardScaler(), MLPClassifier(random_state=18))

# Train the MLP model
mlp_pipeline.fit(X_train, y_train)


# Accuracy on whole data
predictions = mlp_pipeline.predict(X.values)
accuracy = accuracy_score(y,predictions)
print(f"Accuracy on Whole Data: {accuracy*100}%")
print(f"Precision Score: {precision_score(y,predictions)}")
print(f"Recall Score: {recall_score(y,predictions)}")
print(f"F1 Score: {f1_score(y,predictions)}")
plot_confusion_matrix(mlp_pipeline, X.values, y)
plt.title("Confusion Matrix for Whole Data")
plt.show()
print(classification_report(y,predictions))

"""## Comparison Between Neural Networks"""

import seaborn as sns
import matplotlib.pyplot as plt

# Define the models and corresponding accuracies
model = ['Back-Propogation', 'Multi-Layer Perceptron']
acc = [0.5026666522026062, 0.57294288238137305]

# Plot the barplot
plt.figure(figsize=[10, 5], dpi=100)
plt.title('Accuracy Comparison')
plt.xlabel('Algorithms')
plt.ylabel('Accuracy')
sns.barplot(x=model, y=acc)
plt.show()

"""## Total Accuracy Comparisons of all the Models"""

import seaborn as sns
import matplotlib.pyplot as plt

# Define the models and corresponding accuracies
model = ['RandomForestClassifier', 'XGBClassifier', 'LogisticRegression','DecisionTreeClassifier','KNeighborsClassifier',
'Back-Propogation', 'Multi-Layer Perceptron']
acc = [0.90001,0.70461,0.50574,0.89844,0.64939,0.5026666522026062, 0.57294288238137305]

# Plot the barplot
plt.figure(figsize=[20, 10], dpi=150)
plt.title('Accuracy Comparison')
plt.xlabel('Algorithms')
plt.ylabel('Accuracy')
sns.barplot(x=model, y=acc)
plt.show()

