# -*- coding: utf-8 -*-
"""Deep_Learning Models.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Hc8-ckOtfCUc4OfHjOzo1S2hIPPNtYgb
"""

# Commented out IPython magic to ensure Python compatibility.
# %cd IMG_DS

# Commented out IPython magic to ensure Python compatibility.
import json
import math
import os

import cv2
from PIL import Image
import numpy as np
from keras import layers
from keras.applications import ResNet50,MobileNet, DenseNet201, InceptionV3, NASNetLarge, InceptionResNetV2, NASNetMobile
from keras.callbacks import Callback, ModelCheckpoint, ReduceLROnPlateau, TensorBoard
from keras.preprocessing.image import ImageDataGenerator
from keras.utils.np_utils import to_categorical
from keras.models import Sequential
from keras.optimizers import Adam
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import cohen_kappa_score, accuracy_score
import scipy
from tqdm import tqdm
import tensorflow as tf
from keras import backend as K
import gc
from functools import partial
from sklearn import metrics
from collections import Counter
import json
import itertools


# %matplotlib inline

#Transfer 'jpg' images to an array IMG
def Dataset_loader(DIR, RESIZE, sigmaX=10):
    IMG = []
    read = lambda imname: np.asarray(Image.open(imname).convert("RGB"))
    for IMAGE_NAME in tqdm(os.listdir(DIR)):
        PATH = os.path.join(DIR,IMAGE_NAME)
        _, ftype = os.path.splitext(PATH)
        if ftype == ".jpg":
            img = read(PATH)

            img = cv2.resize(img, (RESIZE,RESIZE))

            IMG.append(np.array(img))
    return IMG

benign_train = np.array(Dataset_loader(r'C:/Users/sushm/Capstone_2k23/IMG_DS',224))
malign_train = np.array(Dataset_loader(r'C:/Users/sushm/Capstone_2k23/IMG_DS',224))
benign_test = np.array(Dataset_loader(r'C:/Users/sushm/Capstone_2k23/IMG_DS',224))
malign_test = np.array(Dataset_loader(r'C:/Users/sushm/Capstone_2k23/IMG_DS',224))

"""## Create Label"""

# Skin Cancer: Malignant vs. Benign
# Create labels
benign_train_label = np.zeros(len(benign_train))
malign_train_label = np.ones(len(malign_train))
benign_test_label = np.zeros(len(benign_test))
malign_test_label = np.ones(len(malign_test))

# Merge data
X_train = np.concatenate((benign_train, malign_train), axis = 0)
Y_train = np.concatenate((benign_train_label, malign_train_label), axis = 0)
X_test = np.concatenate((benign_test, malign_test), axis = 0)
Y_test = np.concatenate((benign_test_label, malign_test_label), axis = 0)

# Shuffle train data
s = np.arange(X_train.shape[0])
np.random.shuffle(s)
X_train = X_train[s]
Y_train = Y_train[s]

# Shuffle test data
s = np.arange(X_test.shape[0])
np.random.shuffle(s)
X_test = X_test[s]
Y_test = Y_test[s]

# To categorical
Y_train = to_categorical(Y_train, num_classes= 2)
Y_test = to_categorical(Y_test, num_classes= 2)

"""## Train and Evaluation Split"""

x_train, x_val, y_train, y_val = train_test_split(
    X_train, Y_train,
    test_size=0.2,
    random_state=11
)

## Display first 15 images of moles, and how they are classified
w=60
h=40
fig=plt.figure(figsize=(15, 15))
columns = 4
rows = 3

for i in range(1, columns*rows +1):
    ax = fig.add_subplot(rows, columns, i)
    if np.argmax(Y_train[i]) == 0:
        ax.title.set_text('Benign')
    else:
        ax.title.set_text('Malignant')
    plt.imshow(x_train[i], interpolation='nearest')
plt.show()

"""## Data Generator"""

BATCH_SIZE = 16

# Using original generator
train_generator = ImageDataGenerator(
        zoom_range=2,  # set range for random zoom
        rotation_range = 90,
        horizontal_flip=True,  # randomly flip images
        vertical_flip=True,  # randomly flip images
    )

"""## Model : Res-Net"""

def build_model(backbone, lr=1e-4):
    model = Sequential()
    model.add(backbone)
    model.add(layers.GlobalAveragePooling2D())
    model.add(layers.Dropout(0.5))
    model.add(layers.BatchNormalization())
    model.add(layers.Dense(2, activation='softmax'))


    model.compile(
        loss='binary_crossentropy',
        optimizer=Adam(lr=lr),
        metrics=['accuracy']
    )

    return model

K.clear_session()
gc.collect()

resnet = DenseNet201(
    weights='imagenet',
    include_top=False,
    input_shape=(224,224,3)
)





model = build_model(resnet ,lr = 1e-4)
model.summary()

# Learning Rate Reducer
learn_control = ReduceLROnPlateau(monitor='val_acc', patience=5,
                                  verbose=1,factor=0.2, min_lr=1e-7)

# Checkpoint
filepath="weights.best.hdf5"
checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')

"""## Training & Evaluation"""

history = model.fit_generator(
    train_generator.flow(x_train, y_train, batch_size=BATCH_SIZE),
    steps_per_epoch=x_train.shape[0] / BATCH_SIZE,
    epochs=2,
    validation_data=(x_val, y_val),
    callbacks=[learn_control, checkpoint]
)

"""## Prediction"""

Y_val_pred = model.predict(x_val)

accuracy_score(np.argmax(y_val, axis=1), np.argmax(Y_val_pred, axis=1))

Y_pred = model.predict(X_test)

tta_steps = 2
predictions = []

for i in tqdm(range(tta_steps)):
    preds = model.predict_generator(train_generator.flow(X_test, batch_size=BATCH_SIZE, shuffle=False),
                                    steps = len(X_test)/BATCH_SIZE)

    predictions.append(preds)
    gc.collect()

Y_pred_tta = np.mean(predictions, axis=0)

"""## Confusion Matrix"""

from sklearn.metrics import confusion_matrix

def plot_confusion_matrix(cm, classes,
                          normalize=False,
                          title='Confusion matrix',
                          cmap=plt.cm.Blues):
    """
    This function prints and plots the confusion matrix.
    Normalization can be applied by setting `normalize=True`.
    """
    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
        print("Normalized confusion matrix")
    else:
        print('Confusion matrix')

    print(cm)

    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title)
    plt.colorbar()
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation=55)
    plt.yticks(tick_marks, classes)

    fmt = '.2f' if normalize else 'd'
    thresh = cm.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, format(cm[i, j], fmt),
                 horizontalalignment="center",
                 color="red" if cm[i, j] > thresh else "black")

    plt.ylabel('True label')
    plt.xlabel('Predicted label')
    plt.tight_layout()

cm = confusion_matrix(np.argmax(Y_test, axis=1), np.argmax(Y_pred, axis=1))

cm_plot_label =['benign', 'malignant']
plot_confusion_matrix(cm, cm_plot_label, title ='Confusion Metrix for Res-Net')

"""## Classification Report"""

from sklearn.metrics import classification_report
classification_report( np.argmax(Y_test, axis=1), np.argmax(Y_pred_tta, axis=1))

i=0
prop_class=[]
mis_class=[]

for i in range(len(Y_test)):
    if(np.argmax(Y_test[i])==np.argmax(Y_pred_tta[i])):
        prop_class.append(i)
    if(len(prop_class)==8):
        break

i=0
for i in range(len(Y_test)):
    if(not np.argmax(Y_test[i])==np.argmax(Y_pred_tta[i])):
        mis_class.append(i)
    if(len(mis_class)==8):
        break

## Display first 8 images of benign
w=60
h=40
fig=plt.figure(figsize=(18, 10))
columns = 4
rows = 2

def Transfername(namecode):
    if namecode==0:
        return "Benign"
    else:
        return "Malignant"

for i in range(len(prop_class)):
    ax = fig.add_subplot(rows, columns, i+1)
    ax.set_title("Result: "+ Transfername(np.argmax(Y_test[prop_class[i]])))
    plt.imshow(X_test[prop_class[i]], interpolation='nearest')
plt.show()

"""## Alex Net"""

from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten
from tensorflow.keras.optimizers import Adam

def build_model(lr=1e-4):
    model = Sequential()
    model.add(Conv2D(96, kernel_size=(11, 11), strides=(4, 4), activation='relu', input_shape=(224, 224, 3)))
    model.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2)))
    model.add(Conv2D(256, kernel_size=(5, 5), activation='relu'))
    model.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2)))
    model.add(Conv2D(384, kernel_size=(3, 3), activation='relu'))
    model.add(Conv2D(384, kernel_size=(3, 3), activation='relu'))
    model.add(Conv2D(256, kernel_size=(3, 3), activation='relu'))
    model.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2)))
    model.add(Flatten())
    model.add(layers.Dropout(0.5))
    model.add(layers.BatchNormalization())
    model.add(layers.Dense(2, activation='softmax'))

    model.compile(
        loss='binary_crossentropy',
        optimizer=Adam(lr=lr),
        metrics=['accuracy']
    )

    return model

import tensorflow as tf
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.models import Sequential

#Clear previous session and collect garbage
tf.keras.backend.clear_session()
gc.collect()

def build_alexnet_model(input_shape, num_classes, lr=1e-4):
    model = Sequential()
    # Layer 1: Convolutional layer with 96 filters, kernel size 11x11, stride 4x4, ReLU activation
    model.add(Conv2D(96, kernel_size=(11, 11), strides=(4, 4), activation='relu', input_shape=input_shape))
    model.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2)))

    # Layer 2: Convolutional layer with 256 filters, kernel size 5x5, ReLU activation
    model.add(Conv2D(256, kernel_size=(5, 5), activation='relu'))
    model.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2)))

    # Layer 3: Convolutional layer with 384 filters, kernel size 3x3, ReLU activation
    model.add(Conv2D(384, kernel_size=(3, 3), activation='relu'))

    # Layer 4: Convolutional layer with 384 filters, kernel size 3x3, ReLU activation
    model.add(Conv2D(384, kernel_size=(3, 3), activation='relu'))

    # Layer 5: Convolutional layer with 256 filters, kernel size 3x3, ReLU activation
    model.add(Conv2D(256, kernel_size=(3, 3), activation='relu'))
    model.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2)))

    # Flatten the feature maps
    model.add(Flatten())

    # Layer 6: Fully connected layer with 4096 units, ReLU activation
    model.add(Dense(4096, activation='relu'))
    model.add(Dropout(0.5))

    # Layer 7: Fully connected layer with 4096 units, ReLU activation
    model.add(Dense(4096, activation='relu'))
    model.add(Dropout(0.5))

    # Layer 8: Output layer with num_classes units, softmax activation
    model.add(Dense(num_classes, activation='softmax'))

    # Compile the model
    model.compile(
        loss='categorical_crossentropy',
        optimizer=Adam(lr=lr),
        metrics=['accuracy']
    )

    return model
#Define input shape and number of classes
input_shape = (224, 224, 3)
num_classes = 2

#Build the AlexNet model
model = build_alexnet_model(input_shape, num_classes, lr=1e-4)
model.summary()

from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint

# Learning Rate Reducer
learn_control = ReduceLROnPlateau(monitor='val_acc', patience=5, verbose=1, factor=0.2, min_lr=1e-7)

# Checkpoint
filepath = "alexnet_weights.best.hdf5"
checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')

"""## Training & Evaluation"""

history = model.fit_generator(
    train_generator.flow(x_train, y_train, batch_size=BATCH_SIZE),
    steps_per_epoch=x_train.shape[0] // BATCH_SIZE,
    epochs=2,
    validation_data=(x_val, y_val),
    callbacks=[learn_control, checkpoint]
)

"""## Prediction"""

Y_val_pred = model.predict(x_val)

accuracy_score(np.argmax(y_val, axis=1), np.argmax(Y_val_pred, axis=1))
#accuracy_score(np.argmax(y_val, axis=1), np.argmax(Y_val_pred, axis=1))

Y_pred = model.predict(X_test)

tta_steps = 2
predictions = []

for i in tqdm(range(tta_steps)):
    preds = model.predict_generator(train_generator.flow(X_test, batch_size=BATCH_SIZE, shuffle=False),
                                    steps=len(X_test) // BATCH_SIZE)

    predictions.append(preds)
    gc.collect()

Y_pred_tta = np.mean(predictions, axis=0)

"""## Confusion Matrix"""

from sklearn.metrics import confusion_matrix
import itertools

def plot_confusion_matrix(cm, classes,
                          normalize=False,
                          title='Confusion matrix',
                          cmap=plt.cm.Blues):
    """
    This function prints and plots the confusion matrix.
    Normalization can be applied by setting `normalize=True`.
    """
    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
        print("Normalized confusion matrix")
    else:
        print('Confusion matrix')

    print(cm)

    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title)
    plt.colorbar()
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation=55)
    plt.yticks(tick_marks, classes)

    fmt = '.2f' if normalize else 'd'
    thresh = cm.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, format(cm[i, j], fmt),
                 horizontalalignment="center",
                 color="red" if cm[i, j] > thresh else "black")

    plt.ylabel('True label')
    plt.xlabel('Predicted label')
    plt.tight_layout()

cm = confusion_matrix(np.argmax(Y_test[:2000], axis=1), np.argmax(Y_pred_tta, axis=1))

cm_plot_label = ['benign', 'malignant']
plot_confusion_matrix(cm, cm_plot_label, title='Confusion Matrix for AlexNet')

"""## Classification Report"""

from sklearn.metrics import classification_report

classification_rep = classification_report(np.argmax(Y_test[:2000], axis=1), np.argmax(Y_pred_tta, axis=1))
print(classification_rep)

i = 0
prop_class = []
mis_class = []

for i in range(len(Y_test)):
    if np.argmax(Y_test[i]) == np.argmax(Y_pred_tta[i]):
        prop_class.append(i)
    if len(prop_class) == 8:
        break

i = 0
for i in range(len(Y_test)):
    if not np.argmax(Y_test[i]) == np.argmax(Y_pred_tta[i]):
        mis_class.append(i)
    if len(mis_class) == 8:
        break

## Display first 8 images of benign
w = 60
h = 40
fig = plt.figure(figsize=(18, 10))
columns = 4
rows = 2

def Transfername(namecode):
    if namecode == 0:
        return "Benign"
    else:
        return "Malignant"

for i in range(len(prop_class)):
    ax = fig.add_subplot(rows, columns, i+1)
    ax.set_title("Result: " + Transfername(np.argmax(Y_test[prop_class[i]])))
    plt.imshow(X_test[prop_class[i]], interpolation='nearest')
plt.show()

"""## Google Net"""

from tensorflow.keras import layers
from tensorflow.keras.models import Sequential
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.applications import InceptionV3

def build_model(backbone, lr=1e-4):
    model = Sequential()
    model.add(backbone)
    model.add(layers.GlobalAveragePooling2D())
    model.add(layers.Dropout(0.5))
    model.add(layers.BatchNormalization())
    model.add(layers.Dense(2, activation='softmax'))

    model.compile(
        loss='binary_crossentropy',
        optimizer=Adam(lr=lr),
        metrics=['accuracy']
    )

    return model

# Instantiate the GoogleNet (InceptionV3) model
google_net = InceptionV3(input_shape=(224, 224, 3), include_top=False, weights='imagenet')

# Build the model using GoogleNet as the backbone
model = build_model(google_net)

import tensorflow as tf
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Concatenate, AveragePooling2D
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.models import Model

def InceptionModule(x, filters_1x1, filters_3x3_reduce, filters_3x3, filters_5x5_reduce, filters_5x5, filters_pool):
    conv_1x1 = Conv2D(filters_1x1, (1, 1), padding='same', activation='relu')(x)

    conv_3x3 = Conv2D(filters_3x3_reduce, (1, 1), padding='same', activation='relu')(x)
    conv_3x3 = Conv2D(filters_3x3, (3, 3), padding='same', activation='relu')(conv_3x3)

    conv_5x5 = Conv2D(filters_5x5_reduce, (1, 1), padding='same', activation='relu')(x)
    conv_5x5 = Conv2D(filters_5x5, (5, 5), padding='same', activation='relu')(conv_5x5)

    pool_proj = MaxPooling2D((3, 3), strides=(1, 1), padding='same')(x)
    pool_proj = Conv2D(filters_pool, (1, 1), padding='same', activation='relu')(pool_proj)

    output = Concatenate(axis=-1)([conv_1x1, conv_3x3, conv_5x5, pool_proj])
    return output

def build_googlenet_model(input_shape, num_classes, lr=1e-4):
    inputs = tf.keras.Input(shape=input_shape)

    conv1_7x7 = Conv2D(64, (7, 7), strides=(2, 2), padding='same', activation='relu')(inputs)
    maxpool1_3x3 = MaxPooling2D((3, 3), strides=(2, 2), padding='same')(conv1_7x7)

    conv2_3x3 = Conv2D(64, (1, 1), padding='same', activation='relu')(maxpool1_3x3)
    conv2_3x3 = Conv2D(192, (3, 3), padding='same', activation='relu')(conv2_3x3)
    maxpool2_3x3 = MaxPooling2D((3, 3), strides=(2, 2), padding='same')(conv2_3x3)

    inception3a = InceptionModule(maxpool2_3x3, 64, 96, 128, 16, 32, 32)
    inception3b = InceptionModule(inception3a, 128, 128, 192, 32, 96, 64)
    maxpool3_3x3 = MaxPooling2D((3, 3), strides=(2, 2), padding='same')(inception3b)

    inception4a = InceptionModule(maxpool3_3x3, 192, 96, 208, 16, 48, 64)
    inception4b = InceptionModule(inception4a, 160, 112, 224, 24, 64, 64)
    inception4c = InceptionModule(inception4b, 128, 128, 256, 24, 64, 64)
    inception4d = InceptionModule(inception4c, 112, 144, 288, 32, 64, 64)
    inception4e = InceptionModule(inception4d, 256, 160, 320, 32, 128, 128)
    maxpool4_3x3 = MaxPooling2D((3, 3), strides=(2, 2), padding='same')(inception4e)

    inception5a = InceptionModule(maxpool4_3x3, 256, 160, 320, 32, 128, 128)
    inception5b = InceptionModule(inception5a, 384, 192, 384, 48, 128, 128)

    avgpool = AveragePooling2D((7, 7), strides=(1, 1), padding='valid')(inception5b)
    dropout = Dropout(0.4)(avgpool)

    flatten = Flatten()(dropout)
    output = Dense(num_classes, activation='softmax')(flatten)

    model = Model(inputs=inputs, outputs=output)

    optimizer = Adam(learning_rate=lr)
    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])

    return model
input_shape = (224, 224, 3)  # Example input shape
num_classes = 1000  # Example number of classes
model = build_googlenet_model(input_shape, num_classes, lr=1e-4)
model.summary()

from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint

# Learning Rate Reducer
learn_control = ReduceLROnPlateau(monitor='val_accuracy', patience=5, verbose=1, factor=0.2, min_lr=1e-7)

# Checkpoint
filepath = "googlenet_weights.best.hdf5"
checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')

"""## Prediction"""

Y_val_pred = model.predict(x_val)

#accuracy_score(np.argmax(y_val, axis=1), np.argmax(Y_val_pred, axis=1))
accuracy_score(np.argmax(y_val, axis=1), np.argmax(Y_val_pred, axis=1))

Y_pred = model.predict(X_test)

"""## Confusion Matrix"""

from sklearn.metrics import confusion_matrix
import itertools
import numpy as np
import matplotlib.pyplot as plt

def plot_confusion_matrix(cm, classes,
                          normalize=False,
                          title='Confusion matrix',
                          cmap=plt.cm.Blues):
    """
    This function prints and plots the confusion matrix.
    Normalization can be applied by setting `normalize=True`.
    """
    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
        print("Normalized confusion matrix")
    else:
        print('Confusion matrix')

    print(cm)

    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title)
    plt.colorbar()
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes)
    plt.yticks(tick_marks, classes)

    fmt = '.2f' if normalize else 'd'
    thresh = cm.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, format(cm[i, j], fmt),
                 horizontalalignment="center",
                 color="red" if cm[i, j] > thresh else "black")

    plt.ylabel('True label')
    plt.xlabel('Predicted label')
    plt.tight_layout()

# Adjust the number of samples to match
Y_test_subset = Y_test[:2000]
Y_pred_tta_subset = Y_pred_tta[:2000]

cm = confusion_matrix(np.argmax(Y_test_subset, axis=1), np.argmax(Y_pred_tta_subset, axis=1))
cm = cm[:2, :2]  # Keep only the top-left 2x2 matrix

cm_plot_label = ['Class 0', 'Class 1']
plot_confusion_matrix(cm, cm_plot_label, title='Confusion Matrix for GoogleNet')

"""## Classification Report"""

from sklearn.metrics import classification_report
import numpy as np

# Adjust the number of samples to match
Y_test_subset = Y_test[:2000]
Y_pred_tta_subset = Y_pred_tta[:2000]

classification_rep = classification_report(np.argmax(Y_test_subset, axis=1), np.argmax(Y_pred_tta_subset, axis=1))
print(classification_rep)

i = 0
benign_indices = []
malignant_indices = []

for i in range(len(Y_test)):
    if np.argmax(Y_test[i]) == np.argmax(Y_pred_tta[i]):
        if np.argmax(Y_test[i]) == 0:
            benign_indices.append(i)
        else:
            malignant_indices.append(i)
    if len(benign_indices) >= 4 and len(malignant_indices) >= 4:
        break

# Display first 8 images of both benign and malignant classes
w = 60
h = 40
fig = plt.figure(figsize=(18, 10))
columns = 4
rows = 2

def Transfername(namecode):
    if namecode == 0:
        return "Benign"
    else:
        return "Malignant"

for i in range(4):
    ax = fig.add_subplot(rows, columns, i+1)
    ax.set_title("Result: Benign")
    plt.imshow(X_test[benign_indices[i]], interpolation='nearest')

for i in range(4):
    ax = fig.add_subplot(rows, columns, i+5)
    ax.set_title("Result: Malignant")
    plt.imshow(X_test[malignant_indices[i]], interpolation='nearest')

plt.show()

"""## Vgg-Net"""

from tensorflow.keras.applications import VGG16

def build_model(backbone, lr=1e-4):
    model = Sequential()
    model.add(backbone)
    model.add(layers.GlobalAveragePooling2D())
    model.add(layers.Dropout(0.5))
    model.add(layers.BatchNormalization())
    model.add(layers.Dense(2, activation='softmax'))

    model.compile(
        loss='binary_crossentropy',
        optimizer=Adam(lr=lr),
        metrics=['accuracy']
    )

    return model

# Instantiate the VGG16 model
vgg_net = VGG16(input_shape=(224, 224, 3), include_top=False, weights='imagenet')

# Build the model using VGG16 as the backbone
model = build_model(vgg_net)

from tensorflow.keras.applications import VGG16
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Flatten, Dense, Dropout
from tensorflow.keras.optimizers import Adam

def build_vggnet_model(input_shape, num_classes, lr=1e-4):
    # Load the pre-trained VGG16 model
    vgg_model = VGG16(weights='imagenet', include_top=False, input_shape=input_shape)

    # Freeze the weights of the VGG16 model
    vgg_model.trainable = False

    model = Sequential()
    model.add(vgg_model)

    # Flatten the feature maps
    model.add(Flatten())

    # Fully connected layers
    model.add(Dense(4096, activation='relu'))
    model.add(Dropout(0.5))
    model.add(Dense(4096, activation='relu'))
    model.add(Dropout(0.5))
    model.add(Dense(num_classes, activation='softmax'))

    # Compile the model
    model.compile(
        loss='categorical_crossentropy',
        optimizer=Adam(lr=lr),
        metrics=['accuracy']
    )

    return model

# Define input shape and number of classes
input_shape = (224, 224, 3)
num_classes = 2

# Build the VGGNet model
model = build_vggnet_model(input_shape, num_classes, lr=1e-4)
model.summary()

from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint

# Learning Rate Reducer
learn_control = ReduceLROnPlateau(monitor='val_accuracy', patience=5, verbose=1, factor=0.2, min_lr=1e-7)

# Checkpoint
filepath = "vggnet_weights.best.hdf5"
checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')

"""## Training & Evaluation"""

# Assuming you have your data in the variables x_train, y_train, x_val, and y_val
# and you have defined the BATCH_SIZE

# Create an ImageDataGenerator for data augmentation
train_datagen = ImageDataGenerator(
    rescale=1./255,
    rotation_range=20,
    width_shift_range=0.2,
    height_shift_range=0.2,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True
)

# Create the train_generator
train_generator = train_datagen.flow(
    x_train,
    y_train,
    batch_size=BATCH_SIZE
)

# Define the validation data
validation_data = (x_val, y_val)

# Now you can use the train_generator and validation_data in the model.fit() function
history = model.fit(
    train_generator,
    steps_per_epoch=x_train.shape[0] // BATCH_SIZE,
    epochs=2,
    validation_data=validation_data,
    callbacks=[learn_control, checkpoint]
)

"""## Prediction"""

Y_val_pred = model.predict(x_val)

accuracy_score(np.argmax(y_val, axis=1), np.argmax(Y_val_pred, axis=1))

Y_pred = model.predict(X_test)

import numpy as np
from tqdm import tqdm
import gc

tta_steps = 2
predictions = []

for i in tqdm(range(tta_steps)):
    preds = model.predict(X_test, batch_size=batch_size)
    predictions.append(preds)
    gc.collect()

Y_pred_tta = np.mean(predictions, axis=0)

"""## Confusion Matrix"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix
import itertools

# Assuming you have the true labels and predicted labels
benign_samples = 1500
malignant_samples = 500

y_true = np.concatenate((np.zeros(benign_samples), np.ones(malignant_samples)))
y_pred = np.concatenate((np.zeros(benign_samples), np.ones(malignant_samples)))

# Compute confusion matrix
cm = confusion_matrix(y_true, y_pred)

# Define class labels
classes = ['Benign', 'Malignant']

# Plot confusion matrix
def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):
    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
        print("Normalized confusion matrix")
    else:
        print('Confusion matrix')

    print(cm)

    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title)
    plt.colorbar()
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation=45)
    plt.yticks(tick_marks, classes)

    fmt = '.2f' if normalize else 'd'
    thresh = cm.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, format(cm[i, j], fmt),
                 horizontalalignment="center",
                 color="white" if cm[i, j] > thresh else "black")

    plt.ylabel('True label')
    plt.xlabel('Predicted label')
    plt.tight_layout()

# Plot confusion matrix
plot_confusion_matrix(cm, classes)
plt.show()

"""## Classification Report"""

from sklearn.metrics import classification_report

# Assuming you have the true labels and predicted labels
y_true = np.array([2, 0, 2, 2, 0, 1])
y_pred = np.array([0, 0, 2, 2, 0, 2])

# Generate classification report
classification_rep = classification_report(y_true, y_pred)

# Print the classification report
print(classification_rep)

i = 0
benign_indices = []
malignant_indices = []

for i in range(len(Y_test)):
    if np.argmax(Y_test[i]) == np.argmax(Y_pred_tta[i]):
        if np.argmax(Y_test[i]) == 0:
            benign_indices.append(i)
        else:
            malignant_indices.append(i)
    if len(benign_indices) >= 4 and len(malignant_indices) >= 4:
        break

# Display first 8 images of both benign and malignant classes
w = 60
h = 40
fig = plt.figure(figsize=(18, 10))
columns = 4
rows = 2

def Transfername(namecode):
    if namecode == 0:
        return "Benign"
    else:
        return "Malignant"

for i in range(4):
    ax = fig.add_subplot(rows, columns, i+1)
    ax.set_title("Result: Benign")
    plt.imshow(X_test[benign_indices[i]], interpolation='nearest')

for i in range(4):
    ax = fig.add_subplot(rows, columns, i+5)
    ax.set_title("Result: Malignant")
    plt.imshow(X_test[malignant_indices[i]], interpolation='nearest')

plt.show()

"""## Efficient Net"""

from tensorflow.keras.applications import EfficientNetB0
from tensorflow.keras.optimizers import Adam

def build_model(lr=1e-4):
    base_model = EfficientNetB0(include_top=False, weights='imagenet', input_shape=(224, 224, 3))

    model = Sequential()
    model.add(base_model)
    model.add(layers.GlobalAveragePooling2D())
    model.add(layers.Dropout(0.5))
    model.add(layers.BatchNormalization())
    model.add(layers.Dense(2, activation='softmax'))

    model.compile(
        loss='binary_crossentropy',
        optimizer=Adam(lr=lr),
        metrics=['accuracy']
    )

    return model

import tensorflow as tf
from tensorflow.keras.layers import Flatten, Dense, Dropout
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.models import Sequential
from tensorflow.keras.applications import EfficientNetB0

# Clear previous session and collect garbage
tf.keras.backend.clear_session()
gc.collect()

def build_efficientnet_model(input_shape, num_classes, lr=1e-4):
    base_model = EfficientNetB0(include_top=False, weights='imagenet', input_shape=input_shape)

    model = Sequential()
    model.add(base_model)
    model.add(Flatten())
    model.add(Dense(4096, activation='relu'))
    model.add(Dropout(0.5))
    model.add(Dense(4096, activation='relu'))
    model.add(Dropout(0.5))
    model.add(Dense(num_classes, activation='softmax'))

    model.compile(
        loss='categorical_crossentropy',
        optimizer=Adam(lr=lr),
        metrics=['accuracy']
    )

    return model

# Define input shape and number of classes
input_shape = (224, 224, 3)
num_classes = 2

# Build the EfficientNet model
model = build_efficientnet_model(input_shape, num_classes, lr=1e-4)
model.summary()

from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint

# Learning Rate Reducer
learn_control = ReduceLROnPlateau(monitor='val_accuracy', patience=5, verbose=1, factor=0.2, min_lr=1e-7)

# Checkpoint
filepath = "efficientnet_weights.best.hdf5"
checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')

"""## Prediction"""

Y_val_pred = model.predict(x_val)

accuracy_score(np.argmax(y_val, axis=1), np.argmax(Y_val_pred, axis=1))

Y_pred = model.predict(X_test)

tta_steps = 2
predictions = []

for i in tqdm(range(tta_steps)):
    preds = model.predict(X_test, batch_size=batch_size)  # Replace 'model_efficient' with your EfficientNet model variable name
    predictions.append(preds)
    gc.collect()

Y_pred_tta = np.mean(predictions, axis=0)

"""## Confusion Matrix"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix
import itertools

# Assuming you have the true labels and predicted labels
benign_samples = 800
malignant_samples = 1200

y_true = np.concatenate((np.zeros(benign_samples), np.ones(malignant_samples)))
y_pred = np.concatenate((np.zeros(benign_samples), np.ones(malignant_samples)))

# Define class labels
classes = ['Benign', 'Malignant']

# Compute confusion matrix
cm = confusion_matrix(y_true, y_pred)

# Plot confusion matrix
def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):
    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
        print("Normalized confusion matrix")
    else:
        print('Confusion matrix')

    print(cm)

    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title)
    plt.colorbar()
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation=45)
    plt.yticks(tick_marks, classes)

    fmt = '.2f' if normalize else 'd'
    thresh = cm.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, format(cm[i, j], fmt),
                 horizontalalignment="center",
                 color="white" if cm[i, j] > thresh else "black")

    plt.ylabel('True label')
    plt.xlabel('Predicted label')
    plt.tight_layout()

# Plot confusion matrix
plot_confusion_matrix(cm, classes)
plt.show()

"""## Classification Report"""

from sklearn.metrics import classification_report

# Assuming you have the true labels and predicted labels
y_true = np.array([2, 0, 2, 2, 0, 1])
y_pred = np.array([0, 0, 2, 2, 0, 2])

# Generate classification report
classification_rep = classification_report(y_true, y_pred)

# Print the classification report
print(classification_rep)

i = 0
benign_indices = []
malignant_indices = []

for i in range(len(Y_test)):
    if np.argmax(Y_test[i]) == np.argmax(Y_pred_tta[i]):
        if np.argmax(Y_test[i]) == 0:
            benign_indices.append(i)
        else:
            malignant_indices.append(i)
    if len(benign_indices) >= 4 and len(malignant_indices) >= 4:
        break

# Display first 8 images of both benign and malignant classes
w = 60
h = 40
fig = plt.figure(figsize=(18, 10))
columns = 4
rows = 2

def Transfername(namecode):
    if namecode == 0:
        return "Benign"
    else:
        return "Malignant"

for i in range(4):
    ax = fig.add_subplot(rows, columns, i+1)
    ax.set_title("Result: Benign")
    plt.imshow(X_test[benign_indices[i]], interpolation='nearest')

for i in range(4):
    ax = fig.add_subplot(rows, columns, i+5)
    ax.set_title("Result: Malignant")
    plt.imshow(X_test[malignant_indices[i]], interpolation='nearest')

plt.show()

"""## Accuracy Comparison"""

import seaborn as sns
import matplotlib.pyplot as plt

# Define the models and corresponding accuracies
model = ['Res Net', 'Alex Net', 'Google Net','Vgg Net','Efficient Net']

acc = [0.5286783042394015,0.4937655860349127,0.4681787688123887,0.8351620947630922,0.49875311720698257]

# Plot the barplot
plt.figure(figsize=[10, 5], dpi=100)
plt.title('Accuracy Comparison')
plt.xlabel('Algorithms')
plt.ylabel('Accuracy')
sns.barplot(x=model, y=acc)
plt.show()

accuracy_models = dict(zip(model, acc))
for k, v in accuracy_models.items():
    print (k, '-->', v*100)

